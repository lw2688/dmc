{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "#The method list() takes sequence types and converts them to lists. This is used to convert a given tuple into list\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "#create dictionary of characters and numbers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding (encode cateogrical feature to binary values), so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle the indeces that are shared by both inputs and outputs \n",
    "#for convenience of spliting them into training and test data\n",
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oyed the fastest productivity growth in the g7, but it has slowed across nearly all advanced economi --> e\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 107s - loss: 3.2401 - val_loss: 2.9600\n",
      "----- generating with seed: arly 20th centuries, and any number of eras in which americans were told they could restore past glo\n",
      "----- diversity: 0.5\n",
      "arly 20th centuries, and any number of eras in which americans were told they could restore past glonen is ti e ae d erd tenain  de iepr  is ehn st ed nne r uo t a  sielp e adel on ie in  itfre eshha \n",
      "----- diversity: 1.2\n",
      "arly 20th centuries, and any number of eras in which americans were told they could restore past gloe wlourantupaeh2evtc;yuprks.gn a fiidg ee wkewg oyxi r srdafp0jbssi eiwmt    dtrelniree1luj-oiag,ohr\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 3.0161 - val_loss: 2.9010\n",
      "----- generating with seed: uality has risen in most advanced economies, with that increase most pronounced in the united states\n",
      "----- diversity: 0.5\n",
      "uality has risen in most advanced economies, with that increase most pronounced in the united stateshe  dett m td svea st nea c atme n e   an eo ro  ntt  e h n  ete ie ts lt hd teeeinet  eo ieo gorrt \n",
      "----- diversity: 1.2\n",
      "uality has risen in most advanced economies, with that increase most pronounced in the united statesoodt sts airiu2inrgnsuruem rimasahpie ealihdatniso-tfnhsaue svt rin. n,wks  nfpye  girem,i gid cctbs\n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.9336 - val_loss: 2.8169\n",
      "----- generating with seed: t only thrives when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "\n",
      "----- diversity: 0.5\n",
      "t only thrives when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "ntt aoors se eh nr t on regpinsd  ts ar sn tms ioe eed atein ns reo eartrr woa wir ton  aton is erin\n",
      "----- diversity: 1.2\n",
      "t only thrives when there are rules to guard against systemic failure and ensure fair competition.\n",
      "\n",
      "aogoaon1nnt oai9rscher oioem oouewcsfv fhgie ar wgoncehncm pstdwcehener eihwcd ite iaro rw,aeldu,eny\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.8229 - val_loss: 2.6926\n",
      "----- generating with seed: olicies focused on education are critical both for increasing economic growth and for ensuring that \n",
      "----- diversity: 0.5\n",
      "olicies focused on education are critical both for increasing economic growth and for ensuring that tne arg i iihe i hn aor e oe e or et a or te  omut et nnn aal ahe nn  aert ai oon eet aloe ion nain \n",
      "----- diversity: 1.2\n",
      "olicies focused on education are critical both for increasing economic growth and for ensuring that 5im vee0catt ofod itcrdycmekda e  tniots1n,5il rnvel i2incdisfn  oaowubticn bnte5\n",
      "maytoungonalvcasdu\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.6996 - val_loss: 2.6046\n",
      "----- generating with seed: ning a historic debt default. my successors should not have to fight for emergency measures in a tim\n",
      "----- diversity: 0.5\n",
      "ning a historic debt default. my successors should not have to fight for emergency measures in a tim nriey an te oes tor sh cien the one son er inis one the age rrtairiin  amu rolr th tal my  an the a\n",
      "----- diversity: 1.2\n",
      "ning a historic debt default. my successors should not have to fight for emergency measures in a timgsrne nr0igese e- a t pnav uol0 eon nrrtroam ilas\n",
      "rtsefh  bhpeamdbudv.yztudm nelasstdes tind pecvevd\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.6108 - val_loss: 2.5122\n",
      "----- generating with seed:  of workers and their ability to secure a decent wage. too many potential physicists and engineers s\n",
      "----- diversity: 0.5\n",
      " of workers and their ability to secure a decent wage. too many potential physicists and engineers sont anto peoming thfdt anp ete ceal han ore toe cec ope etee the ase to p.tite no easo or onge, tom \n",
      "----- diversity: 1.2\n",
      " of workers and their ability to secure a decent wage. too many potential physicists and engineers sipwbbecmrrsomrcini,.fimimtofinsd .hexeartud dumatmerroniclshchmfrtorlans s sengels tggbepeot sanpsei\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.5418 - val_loss: 2.4587\n",
      "----- generating with seed:  rate in 50 years; annual deficits cut by nearly three-quarters; and declining carbon emissions.\n",
      "\n",
      "fo\n",
      "----- diversity: 0.5\n",
      " rate in 50 years; annual deficits cut by nearly three-quarters; and declining carbon emissions.\n",
      "\n",
      "fore noin inkelise the and ehe an ane the the and mase and wand the geann n alit odrexsit int arlin th\n",
      "----- diversity: 1.2\n",
      " rate in 50 years; annual deficits cut by nearly three-quarters; and declining carbon emissions.\n",
      "\n",
      "fole. g3lupovbtf leen tonad toet0f, thagra rhays,viith0 , ffoon bisyhemvllenac3 costhe vbermeg aog not\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.4878 - val_loss: 2.4230\n",
      "----- generating with seed: . it makes the top and bottom rungs of the ladder stickierharder to move up and harder to lose your \n",
      "----- diversity: 0.5\n",
      ". it makes the top and bottom rungs of the ladder stickierharder to move up and harder to lose your bale sate an, rone te on iner that amear the bo d porotha gis chel an7 pontt es ce wore mast can the\n",
      "----- diversity: 1.2\n",
      ". it makes the top and bottom rungs of the ladder stickierharder to move up and harder to lose your wpridin ageyscealdiond nunke?rkins s6y tasvean t cwaok -olefn.waalien tor: th suaoli: has 7aon. ifri\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.4526 - val_loss: 2.3883\n",
      "----- generating with seed: re larger for households at the bottom and middle of the income distribution than for those at the t\n",
      "----- diversity: 0.5\n",
      "re larger for households at the bottom and middle of the income distribution than for those at the the tha de the and inemant ine pfond pore oon coant aon he and srore ar ar the the the pares the or o\n",
      "----- diversity: 1.2\n",
      "re larger for households at the bottom and middle of the income distribution than for those at the the k 9op 1mudts\n",
      "eit y polrean ritat dufrtk clve wgeo5,, ?alerstuv tib h\n",
      " phrercalt anmer arnduaginul\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.4207 - val_loss: 2.3626\n",
      "----- generating with seed: d choosing to condemn the system as a whole. americans should debate how best to build on these rule\n",
      "----- diversity: 0.5\n",
      "d choosing to condemn the system as a whole. americans should debate how best to build on these rule the eron tor the ar are the s eropres on tho rithe the than nond or irise thans ur core winle the s\n",
      "----- diversity: 1.2\n",
      "d choosing to condemn the system as a whole. americans should debate how best to build on these rule taed ipkre are-gandis. ph airlduct.\n",
      "loros\n",
      ",ombfi; le\n",
      "irtey, th hhe 8acmifns th uilcsofiss pci1mf. g\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.3890 - val_loss: 2.3311\n",
      "----- generating with seed:  productivity growth, combating rising inequality, ensuring that everyone who wants a job can get on\n",
      "----- diversity: 0.5\n",
      " productivity growth, combating rising inequality, ensuring that everyone who wants a job can get on reog on the greiten the rore re pore arure on umerithe sate or inomer ans eresseres an wive ans the\n",
      "----- diversity: 1.2\n",
      " productivity growth, combating rising inequality, ensuring that everyone who wants a job can get oniwl zve2. cond,s.nma. tacur w1aqrand, tvestd bont ecransw-fperricaminostouqirsero he exumdpimp mhvat\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.3583 - val_loss: 2.3140\n",
      "----- generating with seed: , it is 26. people joining or rejoining the workforce in a strengthening economy have offset ageing \n",
      "----- diversity: 0.5\n",
      ", it is 26. people joining or rejoining the workforce in a strengthening economy have offset ageing the the e and ane fore thar the be the thar the rone mec arecore cor pore the fer the the papders an\n",
      "----- diversity: 1.2\n",
      ", it is 26. people joining or rejoining the workforce in a strengthening economy have offset ageing wqobatop: 1f,uros osement coy thas epe moat shamgreorff curadels fvengurcai bb be51icbiy-ss is mover\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 100s - loss: 2.3266 - val_loss: 2.2885\n",
      "----- generating with seed: hould play a critical role. they help workers get a bigger slice of the pie but they need to be flex\n",
      "----- diversity: 0.5\n",
      "hould play a critical role. they help workers get a bigger slice of the pie but they need to be flexting the orotirn on the the the aul thise wos meritican the rode ton the are ane the that shere on t\n",
      "----- diversity: 1.2\n",
      "hould play a critical role. they help workers get a bigger slice of the pie but they need to be flexsulils-ths \n",
      "estene ag oic the fond bali tprnotdicbng tuit ivaele sof kont jemtecsonned ducte mes, an\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.3029 - val_loss: 2.2803\n",
      "----- generating with seed:  increased inequalityit diminishes upward mobility. it makes the top and bottom rungs of the ladder \n",
      "----- diversity: 0.5\n",
      " increased inequalityit diminishes upward mobility. it makes the top and bottom rungs of the ladder sorising the  art reane in cominging the fore in d one tor an protien co fer the the ing the anding \n",
      "----- diversity: 1.2\n",
      " increased inequalityit diminishes upward mobility. it makes the top and bottom rungs of the ladder te oert oapwlce sout u1: pecrloaiest, mhaser6 otelpes bhep erobodterndjceons. mf andp wigldeyeno9 t \n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.2826 - val_loss: 2.2567\n",
      "----- generating with seed: s and suicides among non-college-educated americansthe group where labour-force participation has fa\n",
      "----- diversity: 0.5\n",
      "s and suicides among non-college-educated americansthe group where labour-force participation has fat in the sis mor ercor an of porleng. the wasd the the the le peronge mere best the the fun the are \n",
      "----- diversity: 1.2\n",
      "s and suicides among non-college-educated americansthe group where labour-force participation has fardn anls hat 2f wfaaldomy tist es.gumelie pucticcmwitijh. sfenalvecnansing lut robningeted, bicomoty\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.2515 - val_loss: 2.2393\n",
      "----- generating with seed:  to innovating in the real economy. and the financial crisis of 2008 only seemed to increase the iso\n",
      "----- diversity: 0.5\n",
      " to innovating in the real economy. and the financial crisis of 2008 only seemed to increase the isond ar ofen ane dice and oand the anaricat insteas tha corons we the aing the tict and bising to chep\n",
      "----- diversity: 1.2\n",
      " to innovating in the real economy. and the financial crisis of 2008 only seemed to increase the isoull ene the uerpesenes igalencrade y -oallplijas nek:oe th,e. ta, hhcouanizn arcite tite thanco y au\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.2331 - val_loss: 2.2246\n",
      "----- generating with seed: aining.\n",
      "\n",
      "lifting productivity and wages also depends on creating a global race to the top in rules f\n",
      "----- diversity: 0.5\n",
      "aining.\n",
      "\n",
      "lifting productivity and wages also depends on creating a global race to the top in rules fas the pore and more the pore the to cans es wer the ar corebis more more ace on int are ald ane ed \n",
      "----- diversity: 1.2\n",
      "aining.\n",
      "\n",
      "lifting productivity and wages also depends on creating a global race to the top in rules fan goud af noty satge emenboundencbag-eovdrtm rf ind ty. the tue moev an. le fivist of beemsulte bit\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 97s - loss: 2.2111 - val_loss: 2.2099\n",
      "----- generating with seed: again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate concerns about long-term economic forces.\n",
      "----- diversity: 0.5\n",
      "again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate concerns about long-term economic forces. eaming that dongrmens mant and und ant and besing the the nount and toring the for con bes won the \n",
      "----- diversity: 1.2\n",
      "again.\n",
      "\n",
      "but some of the discontent is rooted in legitimate concerns about long-term economic forces. jonlios, e bomito.\n",
      "\n",
      "\n",
      " inpricd gus vad the wory eesoms, tiin iminge? uutho overomonas ulte-. tor on \n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1871 - val_loss: 2.1895\n",
      "----- generating with seed: etreat into old, closed-off economies or press forward, acknowledging the inequality that can come w\n",
      "----- diversity: 0.5\n",
      "etreat into old, closed-off economies or press forward, acknowledging the inequality that can come wor the more the roste bicandes an the rester for and peomsing to prorting sinc sest of encore the br\n",
      "----- diversity: 1.2\n",
      "etreat into old, closed-off economies or press forward, acknowledging the inequality that can come wor6croutholy pe pomenonikg sce0agt th paagpeserhed tohersses inswert 12 7h, licn\n",
      "imot erabl finin ma\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1668 - val_loss: 2.1879\n",
      "----- generating with seed:  the financial crisis. but it has also been caused by self-imposed constraints: an anti-tax ideology\n",
      "----- diversity: 0.5\n",
      " the financial crisis. but it has also been caused by self-imposed constraints: an anti-tax ideology whe decanti an thate and the and sision the the prosstes for the ale tor tur are and the perint the\n",
      "----- diversity: 1.2\n",
      " the financial crisis. but it has also been caused by self-imposed constraints: an anti-tax ideology beld mess thec r0prerty agcgrew6t clareibs, iod 2o lhi7g.\n",
      "\n",
      "\n",
      "mbme loftiting omeand-alry,,thogva sfor\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 99s - loss: 2.1512 - val_loss: 2.1684\n",
      "----- generating with seed: ugee sentiment expressed by some americans today echoes nativist lurches of the pastthe alien and se\n",
      "----- diversity: 0.5\n",
      "ugee sentiment expressed by some americans today echoes nativist lurches of the pastthe alien and se pertiming wond decaning that soficcnome the prane an ingreced sor the sout th ehald the sacing sede\n",
      "----- diversity: 1.2\n",
      "ugee sentiment expressed by some americans today echoes nativist lurches of the pastthe alien and sesecen eno whire voe sos ild hhergue sunlust. butias.\n",
      "athos halkentgucnovical thelegntiry onstbicant,\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 98s - loss: 2.1299 - val_loss: 2.1658\n",
      "----- generating with seed: rowth is broadly based. a world in which 1 of humanity controls as much wealth as the other 99 will \n",
      "----- diversity: 0.5\n",
      "rowth is broadly based. a world in which 1 of humanity controls as much wealth as the other 99 will alle tho longer condent tha thof to the pand were of rear beore the tor the and an more and sove the\n",
      "----- diversity: 1.2\n",
      "rowth is broadly based. a world in which 1 of humanity controls as much wealth as the other 99 will 155-se ox fbrrirnges eaccom s0.ptiic.iomut in ther dollr1. scon  r0eve the by altc0gr veve oworcento\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 117s - loss: 2.1081 - val_loss: 2.1513\n",
      "----- generating with seed: ne reason why todays ceo is now paid over 250-times more.\n",
      "\n",
      "economies are more successful when we clo\n",
      "----- diversity: 0.5\n",
      "ne reason why todays ceo is now paid over 250-times more.\n",
      "\n",
      "economies are more successful when we close the bon wever ineress the lover bo the tof the for the prast en perate the rase poot and in under\n",
      "----- diversity: 1.2\n",
      "ne reason why todays ceo is now paid over 250-times more.\n",
      "\n",
      "economies are more successful when we clofhe-smeck uldhast amerig. s anlidlap lowothh graundrisinto lost tons cun parp proptens moew mes alt \n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 120s - loss: 2.0946 - val_loss: 2.1431\n",
      "----- generating with seed: al debate.\n",
      "\n",
      "this is the paradox that defines our world today. the world is more prosperous than ever\n",
      "----- diversity: 0.5\n",
      "al debate.\n",
      "\n",
      "this is the paradox that defines our world today. the world is more prosperous than everance the portent of the incoun be and on mere an the or are stet cariste in the and ande pingisimisi\n",
      "----- diversity: 1.2\n",
      "al debate.\n",
      "\n",
      "this is the paradox that defines our world today. the world is more prosperous than everiss waq alpin at helthemo d helres orot ia roy craiiha canoucethe pootect ner.\n",
      "\n",
      "ip4iostthinslifpitie\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 106s - loss: 2.0795 - val_loss: 2.1243\n",
      "----- generating with seed: ilar to a discontent spreading throughout the world, often manifested in scepticism towards internat\n",
      "----- diversity: 0.5\n",
      "ilar to a discontent spreading throughout the world, often manifested in scepticism towards internate and anding tion allide the and the fore fun the the prondes and arereasing and ancesture and and b\n",
      "----- diversity: 1.2\n",
      "ilar to a discontent spreading throughout the world, often manifested in scepticism towards internat or alh-ye0icttosr-torq,ails\n",
      " fouctul: ay toren, dy someasily. sf euredel oncreane io d ass wrhteos \n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 114s - loss: 2.0571 - val_loss: 2.1231\n",
      "----- generating with seed: ll. economies are more successful when we close the gap between rich and poor and growth is broadly \n",
      "----- diversity: 0.5\n",
      "ll. economies are more successful when we close the gap between rich and poor and growth is broadly and inst and arting the eronse duchere and ol eror ameroce the the prows the be proster for the and \n",
      "----- diversity: 1.2\n",
      "ll. economies are more successful when we close the gap between rich and poor and growth is broadly alslernger  herid at chati nu. inimsstion aqies.\n",
      "\n",
      "uncralie ta pare re holdy.\n",
      "\n",
      "l mophe minp sucit ves\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 119s - loss: 2.0367 - val_loss: 2.1272\n",
      "----- generating with seed:  in good times to expand support for the economy when needed and to meet our long-term obligations t\n",
      "----- diversity: 0.5\n",
      " in good times to expand support for the economy when needed and to meet our long-term obligations the incoanon as to the reqaele the sonded cant amer. an the dothes and or and reprone the anforting t\n",
      "----- diversity: 1.2\n",
      " in good times to expand support for the economy when needed and to meet our long-term obligations tiig orvermedhiu; thaingsing harteand tiry issune. thoul greanstused gfompentigvons finss-hmre3. asth\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 2.0234 - val_loss: 2.1107\n",
      "----- generating with seed: cil of economic advisers. so, i will keep pushing for congress to pass the trans-pacific partnership\n",
      "----- diversity: 0.5\n",
      "cil of economic advisers. so, i will keep pushing for congress to pass the trans-pacific partnership the porollo as andereng the arereace and that exont of the pronters the and race cop thes some but \n",
      "----- diversity: 1.2\n",
      "cil of economic advisers. so, i will keep pushing for congress to pass the trans-pacific partnershipy queptipmif whaneder conress ecneper.\n",
      "misyr widecanled. bmonten. um aliul anon solkitgrt an abe ouv\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 120s - loss: 2.0062 - val_loss: 2.1044\n",
      "----- generating with seed: mmunity colleges, proven job-training models and help finding new jobs would assist. so would making\n",
      "----- diversity: 0.5\n",
      "mmunity colleges, proven job-training models and help finding new jobs would assist. so would making and ald the ald ceante and to proating tha ntanmere and dobiting ow share the wor thar and the ress\n",
      "----- diversity: 1.2\n",
      "mmunity colleges, proven job-training models and help finding new jobs would assist. so would making ricad sours, we dopatw if ress alngres oisthe sionn wos wordide prosomes, cvetly am ibantemles fort\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 105s - loss: 1.9952 - val_loss: 2.0963\n",
      "----- generating with seed: ing a historic debt default. my successors should not have to fight for emergency measures in a time\n",
      "----- diversity: 0.5\n",
      "ing a historic debt default. my successors should not have to fight for emergency measures in a time an the urcand the tor ans resuincan und sould corston the bote in and cave of the breange and recon\n",
      "----- diversity: 1.2\n",
      "ing a historic debt default. my successors should not have to fight for emergency measures in a time, tith eanqugeen, peor mucs apliting ainggep canteit etonumsurathictheforg.d ratises falrl. thast om\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 105s - loss: 1.9683 - val_loss: 2.1160\n",
      "----- generating with seed: est rates, fiscal policy must play a bigger role in combating future downturns; monetary policy shou\n",
      "----- diversity: 0.5\n",
      "est rates, fiscal policy must play a bigger role in combating future downturns; monetary policy should and ans instresing the forcens and insurees dost rice arled corencal patis that dofit in the arde\n",
      "----- diversity: 1.2\n",
      "est rates, fiscal policy must play a bigger role in combating future downturns; monetary policy shour fomer cexiti2n onolst coumed2 ins wist thec. oupllicalys amprovitann, covallod mofid eolhdloliting\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 108s - loss: 1.9585 - val_loss: 2.0875\n",
      "----- generating with seed: terity on the economy prematurely by threatening a historic debt default. my successors should not h\n",
      "----- diversity: 0.5\n",
      "terity on the economy prematurely by threatening a historic debt default. my successors should not has enommed and in pering the buonder the prowing ses anderis the  ander and andingsto love the are a\n",
      "----- diversity: 1.2\n",
      "terity on the economy prematurely by threatening a historic debt default. my successors should not ho deud of thol toot of mmene-iss. ive lage ma-refitacem pricy co por deam obrentx yxitl co gul 17, a\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 137s - loss: 1.9347 - val_loss: 2.0936\n",
      "----- generating with seed: finally, the financial crisis painfully underscored the need for a more resilient economy, one that \n",
      "----- diversity: 0.5\n",
      "finally, the financial crisis painfully underscored the need for a more resilient economy, one that dechives as recand and ardes tore forithe and the part enoume that workers the the forles more singu\n",
      "----- diversity: 1.2\n",
      "finally, the financial crisis painfully underscored the need for a more resilient economy, one that tbae , and oupsortes of patlfe pofting lecaliy. ty thes sutes., thenestre thanomkess mopr sunanity t\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 175s - loss: 1.9197 - val_loss: 2.0963\n",
      "----- generating with seed: improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financial crisis pai\n",
      "----- diversity: 0.5\n",
      "improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financial crisis paining resasted the as the and as portura the frat of the were the the part ercanes ace mong ores in t\n",
      "----- diversity: 1.2\n",
      "improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finally, the financial crisis paining, 7res vhomuclise incens fomtill\n",
      "ther thewy sa baint loen to y com.sinsism copreatiingre vicedss\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 132s - loss: 1.8989 - val_loss: 2.0911\n",
      "----- generating with seed: lth insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by \n",
      "----- diversity: 0.5\n",
      "lth insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by to exering the ander ande buting the seat he see the resteres, enous and the prougros of the soress \n",
      "----- diversity: 1.2\n",
      "lth insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by bisgmiveng..\n",
      "iostededical amiact. rabusivisly ans-lanlivetisunsquifiresuul, andimnteer.\n",
      "fs to is to \n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 1.8856 - val_loss: 2.0939\n",
      "----- generating with seed: s administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need to\n",
      "----- diversity: 0.5\n",
      "s administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need to e parme. wo dere the resinces and bate in ratising the ancerine more more fursereconony the arderti\n",
      "----- diversity: 1.2\n",
      "s administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need touudres cnsutrise salghel liven in zins uot hllleyeds s pove revessaonh ase thablcs,, ye bhedktun wml\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 104s - loss: 1.8722 - val_loss: 2.0829\n",
      "----- generating with seed: e offset ageing and retiring baby-boomers since the end of 2013, stabilising the participation rate \n",
      "----- diversity: 0.5\n",
      "e offset ageing and retiring baby-boomers since the end of 2013, stabilising the participation rate man andering the forst reall cons in the of rofeen the toe raole an tor atling that on buris and te \n",
      "----- diversity: 1.2\n",
      "e offset ageing and retiring baby-boomers since the end of 2013, stabilising the participation rate corbale-fem boredment wvakbs bapencpiting. enfurbis. momksesm proodt cecoraican ios ailisca dutaleir\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 110s - loss: 1.8533 - val_loss: 2.0828\n",
      "----- generating with seed: e group or idea that was threatening america under control. we overcame those fears and we will agai\n",
      "----- diversity: 0.5\n",
      "e group or idea that was threatening america under control. we overcame those fears and we will againaste cane for the ame of browats the all bate suncentically alsen conomis can the on and then more \n",
      "----- diversity: 1.2\n",
      "e group or idea that was threatening america under control. we overcame those fears and we will agaicsoagi-ssriledges sudealing weudlom for ichurdes fepwr;iss at w3kinen nhaggr,anss prodetthen for mz.\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 112s - loss: 1.8357 - val_loss: 2.0783\n",
      "----- generating with seed: nearly all advanced economies see chart 1. without a faster-growing economy, we will not be able to \n",
      "----- diversity: 0.5\n",
      "nearly all advanced economies see chart 1. without a faster-growing economy, we will not be able to prorus and the portt rithe the prowlens in that enormating the fort of more rome the the preat in re\n",
      "----- diversity: 1.2\n",
      "nearly all advanced economies see chart 1. without a faster-growing economy, we will not be able to mhahist deapel and raye avl connoomidisas wicundabiat fntipldiy ss wian docysald mo tevjmoict toip l\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 112s - loss: 1.8155 - val_loss: 2.0835\n",
      "----- generating with seed: e auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than even presiden\n",
      "----- diversity: 0.5\n",
      "e auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than even presidenticingrogitg menting an ingreaning siningritions the artere the sucins wo ce fors ol the probett anc\n",
      "----- diversity: 1.2\n",
      "e auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than even presidenialy rabeen ou dwirnmed,d. innalpucliog hive, mes, avery. hal eresur ermaparideng thabedy, eurcom m\n",
      "\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 123s - loss: 1.8090 - val_loss: 2.0805\n",
      "----- generating with seed: hat, for most americans, never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the forces of \n",
      "----- diversity: 0.5\n",
      "hat, for most americans, never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the forces of in walle can the poupling and merestent and ementeranist andere in the the the furles more the for a\n",
      "----- diversity: 1.2\n",
      "hat, for most americans, never existed at all? \n",
      "\n",
      "its true that a certain anxiety over the forces of wicg someceingee a norallere rijectmectars asmanc ofingas izen ajoins wizal fael holdjy ce2s cincure\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 120s - loss: 1.7812 - val_loss: 2.0809\n",
      "----- generating with seed: re not new but just as the child in a slum can see the skyscraper nearby, technology allows anyone w\n",
      "----- diversity: 0.5\n",
      "re not new but just as the child in a slum can see the skyscraper nearby, technology allows anyone wo lcon the tor ins ant recones ald meninesty ces breats on ingerens the the prives fur werhur chmine\n",
      "----- diversity: 1.2\n",
      "re not new but just as the child in a slum can see the skyscraper nearby, technology allows anyone whe ureasd moct one. more of cetheres fersicclione tare\n",
      " auprissommy havp to bike. is cors lark a0 r \n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 111s - loss: 1.7657 - val_loss: 2.0787\n",
      "----- generating with seed: everyone who wants a job can get one and building a resilient economy thats primed for future growth\n",
      "----- diversity: 0.5\n",
      "everyone who wants a job can get one and building a resilient economy thats primed for future growth and shalican sican wes buturing mand patinititn sinnted, in wive and colestins the and in the fare \n",
      "----- diversity: 1.2\n",
      "everyone who wants a job can get one and building a resilient economy thats primed for future growth all dinet veve eadinc as ifrinangy ling. fovetysith worda, at tha tory the meyrequing the edusnawit\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 111s - loss: 1.7486 - val_loss: 2.0918\n",
      "----- generating with seed: ipartisan support would also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "fina\n",
      "----- diversity: 0.5\n",
      "ipartisan support would also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "fination with the resustec innowec anden the proprest the onersed and the instore for more the prowits a\n",
      "----- diversity: 1.2\n",
      "ipartisan support would also improve participation, if enacted.\n",
      "\n",
      "building a sturdier foundation\n",
      "finalby, a love nlecinisa havt opdlllbtoes atel lo17r ther fuchemulsory we relute that far ees lou-dlkuc\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 120s - loss: 1.7282 - val_loss: 2.0864\n",
      "----- generating with seed: ss, we aspire to it and admire those who achieve it. in fact, weve often accepted more inequality th\n",
      "----- diversity: 0.5\n",
      "ss, we aspire to it and admire those who achieve it. in fact, weve often accepted more inequality the and the ryserthe proties tor eres the secones a wer the prowth so the prowes mant emering the ingr\n",
      "----- diversity: 1.2\n",
      "ss, we aspire to it and admire those who achieve it. in fact, weve often accepted more inequality thfiblithed wusthol alces ty moio gaedg pavmuras if polaste nno inesrity refibde. atob incunesmmidinn,\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 127s - loss: 1.7147 - val_loss: 2.1026\n",
      "----- generating with seed: he mid-1800s, the anti-asian sentiment in the late 19th and early 20th centuries, and any number of \n",
      "----- diversity: 0.5\n",
      "he mid-1800s, the anti-asian sentiment in the late 19th and early 20th centuries, and any number of the are ac inco a corter bat ofurichis to wer chardes to har cingrent on the werd as requering the a\n",
      "----- diversity: 1.2\n",
      "he mid-1800s, the anti-asian sentiment in the late 19th and early 20th centuries, and any number of ifucaels meatt ix pricitican sysers be renedst.amicoush in lewautune furlurs boogbudsst d. esdaco., \n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 113s - loss: 1.7053 - val_loss: 2.0871\n",
      "----- generating with seed: , the top 1 of american families received 7 of all after-tax income. by 2007, that share had more th\n",
      "----- diversity: 0.5\n",
      ", the top 1 of american families received 7 of all after-tax income. by 2007, that share had more the ingreate the tore the anderiss american whe whed the ployees the prorten in of imariens in the wiv\n",
      "----- diversity: 1.2\n",
      ", the top 1 of american families received 7 of all after-tax income. by 2007, that share had more the hexmdet ouk in but were talusipat. is recsomes, entiflm ons rolkery mehic-shatitldy cearcs baneto \n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 116s - loss: 1.6847 - val_loss: 2.0916\n",
      "----- generating with seed: oductivity growth. over the past decade, america has enjoyed the fastest productivity growth in the \n",
      "----- diversity: 0.5\n",
      "oductivity growth. over the past decade, america has enjoyed the fastest productivity growth in the for erors the bestriss and bitilly and censonte and anditising that werkes the ther and prevering th\n",
      "----- diversity: 1.2\n",
      "oductivity growth. over the past decade, america has enjoyed the fastest productivity growth in the and dyparlives mosh btiey y coplimate that that bet virt grimins the. the, ercast wervestianbiipdica\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 111s - loss: 1.6687 - val_loss: 2.0887\n",
      "----- generating with seed: americas economy is an enormously complicated mechanism. as appealing as some more radical reforms c\n",
      "----- diversity: 0.5\n",
      "americas economy is an enormously complicated mechanism. as appealing as some more radical reforms cor tore the the part derice so the for fur en and torer and more buting the neand, the provinto itit\n",
      "----- diversity: 1.2\n",
      "americas economy is an enormously complicated mechanism. as appealing as some more radical reforms coromuths thimulc rovudcing-secuets jun trou1 as ercour.\n",
      "ast pat  eins dan tha pourtercong eeths fiou\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 109s - loss: 1.6523 - val_loss: 2.1038\n",
      "----- generating with seed: mises a return to a past that is not possible to restoreand that, for most americans, never existed \n",
      "----- diversity: 0.5\n",
      "mises a return to a past that is not possible to restoreand that, for most americans, never existed ta eres. and buting more acsinct for the are and bititiss of an s endery as in that econemy that for\n",
      "----- diversity: 1.2\n",
      "mises a return to a past that is not possible to restoreand that, for most americans, never existed in what wos incort tinktoy.\n",
      "\n",
      "thi ogatirm dusmitmera hor sthle ant in amerlicenod ut hor betylo stane\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
